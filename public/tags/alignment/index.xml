<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alignment on Notes from the Wired</title>
    <link>http://localhost:1313/tags/alignment/</link>
    <description>Recent content in Alignment on Notes from the Wired</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Jun 2024 16:36:33 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/alignment/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Training language models to follow instructions with human feedback</title>
      <link>http://localhost:1313/paper-summary/training_language_models_to_follow_instructions/</link>
      <pubDate>Mon, 05 Feb 2024 06:45:55 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/training_language_models_to_follow_instructions/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Training language models to follow instructions with human feedback&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.02155&#34;&gt;https://arxiv.org/abs/2203.02155&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 4. March 2022&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; NLP, LLM, Instruction, Alignment&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;This paper introduces a novel technique, instruction fine-tuning, to enhance the alignment of Language Models (LLM) with user instructions. The approach utilizes reinforcement learning from human feedback (RLHF) and demonstrates its effectiveness by applying it to GPT-3.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Language Models (LLM) can be prompted for natural language tasks, but often, the results deviate from the given instructions. This misalignment occurs because LLMs are trained to predict the next token in their input rather than to follow specific tasks. To address this issue, the authors propose instruction fine-tuning, employing reinforcement learning from human feedback (RLHF) to better align the model&amp;rsquo;s objective with the intended task. This technique utilizes human preferences as a reward signal to fine-tune the models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Constitutional AI: Harmlessness from AI Feedback</title>
      <link>http://localhost:1313/paper-summary/constitutional_ai_harmlessness_from_ai_feedback/</link>
      <pubDate>Mon, 05 Feb 2024 06:15:42 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/constitutional_ai_harmlessness_from_ai_feedback/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Constitutional AI: Harmlessness from AI Feedback&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.08073&#34;&gt;https://arxiv.org/abs/2212.08073&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 15. December 2022&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt;  LLM, Alignment, fine-tuning&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;As AI models become more advanced, the need for alignment with human goals, specifically in terms of harmlessness and helpfulness, becomes crucial. Constitutional AI presents a model aligned through a constitution, employing both supervised and RL training methodologies.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;The authors aim to train an AI model that is both harmless and helpful, acknowledging the impracticality of manual supervision and advocating for an automated approach.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Self-Instruct: Aligning Language Models with Self-Generated Instructions</title>
      <link>http://localhost:1313/paper-summary/self_instruct__aligning_language_models/</link>
      <pubDate>Fri, 02 Feb 2024 11:02:28 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/self_instruct__aligning_language_models/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Self-Instruct: Aligning Language Models with Self-Generated Instructions&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;https://arxiv.org/abs/2212.10560&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 20. December 2022&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; LLM, Alignment, Instruction&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;Language Models (LLM) that are &amp;ldquo;instruction tuned&amp;rdquo; have demonstrated excellent raw performance and adherence to instructions. However, instruction tuning typically requires substantial amounts of human-labeled data. This paper introduces Self-Instruct, a technique for instruction tuning a LLM without relying extensively on human-labeled data.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Recent advancements in Language Models (LLM) have significantly enhanced their performance, attributed to the use of pre-trained LLM and subsequent fine-tuning (as seen in the InstructGPT paper) on human-written instructional data (e.g., SUPER-NATURALINSTRUCTIONS). The challenge lies in the impracticality of collecting large datasets, often lacking diversity in the provided instructions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</title>
      <link>http://localhost:1313/paper-summary/chain_of_thought_prompting_elicits_reasoning_in_llms/</link>
      <pubDate>Thu, 01 Feb 2024 13:16:08 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/chain_of_thought_prompting_elicits_reasoning_in_llms/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Chain-of-Thought Prompting Elicits Reasoning in Large Language Models&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.11903&#34;&gt;https://arxiv.org/abs/2201.11903&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 28. January 2022&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; NLP, LLM, prompting&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;The paper explores the concept of Chain of Thought, a sequence of intermediate reasoning steps that significantly enhances the reasoning abilities of Large Language Models (LLMs).&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Chain of thought is an combination of two key ideas:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Arithmetic reasoning, can benefit from using some kind of rationale which lead to the final answer.&lt;/li&gt;&#xA;&lt;li&gt;LLM&amp;rsquo;s have the ability of in context learning, this means without changing their parameters, by using multiple examples in their prompts.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;In essence, Chain of Thought represents a series of reasoning steps in natural language leading to the solution of a given task.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
