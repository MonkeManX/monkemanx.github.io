<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on </title>
    <link>http://localhost:1313/tags/llm/</link>
    <description>Recent content in LLM on </description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Sep 2024 23:03:20 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Towards LLM-based Software Engineering Agents</title>
      <link>http://localhost:1313/articles/llm_based_software_engineering_agents/</link>
      <pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/articles/llm_based_software_engineering_agents/</guid>
      <description>&lt;style&gt;&#xA;    .info {&#xA;        background-color: #e6f7ff;  &#xA;        padding: 15px;  &#xA;        border-radius: 5px;  &#xA;        margin-bottom: 15px;  &#xA;    }&#xA;&#xA;    .info-title {&#xA;        font-size: 20px;  &#xA;        margin-bottom: 10px;  &#xA;        color: #000080;  &#xA;        font-weight: bold;  &#xA;    }&#xA;&lt;/style&gt;&#xA;&#xA;&lt;div class=&#34;info&#34;&gt;&#xA;    &lt;div class=&#34;info-title&#34;&gt;Background&lt;/div&gt;&#xA;    &lt;p&gt;I wrote this article back in uni as part of the seminar, &amp;ldquo;Interactive-Learning&amp;rdquo;. This seminar was done as a group of two, so not all parts in here are written by me.&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;h1 id=&#34;1-from-llms-to-llm-agents&#34;&gt;1. From LLMs to LLM Agents&lt;/h1&gt;&#xA;&lt;p&gt;The advent of ChatGPT in late 2022 kindled an unprecedented hype in Artificial Intelligence, and specifically Large Language Models (LLMs). Ever since the first version of ChatGPT, which was powered by GPT-3, came out, LLMs have evolved a lot. By 2024, research on LLM agents&amp;mdash;LLMs that are enabled to autonomously interact with their environment and perform complex tasks&amp;mdash;has emerged as a critical area of investigation. In this report, we want to introduce a special flavor of LLM agents, namely &lt;em&gt;coding agents&lt;/em&gt;. Coding agents are, as the name suggests, LLM agents for code generation and software engineering more generally. Analyzing this type of LLM agents is particularly advantageous because the correctness of code can be formally verified. This verifiability simplifies creating high-quality benchmarks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Agentless: Demystifying LLM-based Software Engineering Agents</title>
      <link>http://localhost:1313/paper-summary/agentless/</link>
      <pubDate>Sat, 24 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/agentless/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Agentless: Demystifying LLM-based Software Engineering Agents&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2407.01489&#34;&gt;https://arxiv.org/abs/2407.01489&lt;/a&gt; &lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 1. Jurly 2024&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; LLM, Code-generation&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;  &lt;br&gt;&#xA;Recently, there have been made many advancements in the field of autonomous software development using large language models (LLMs). Most of this progress has been driven by &lt;em&gt;agents&lt;/em&gt;. These agents are equipped with tools that allow them to run commands, observe the codebase, and plan future actions. However, these agents come with significant complexity. This paper poses the question: &lt;em&gt;Do we really need all this complexity?&lt;/em&gt;&#xA;&lt;em&gt;Agentless&lt;/em&gt; is an approach to solving software development tasks without the use of agents. It operates in a two-phase process: identifying the buggy code and then repairing it. &lt;em&gt;Agentless&lt;/em&gt; is cost-effective to execute and achieves a high score of 27% on SWE-Bench.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering</title>
      <link>http://localhost:1313/paper-summary/swe_agent_llm_software_engineering/</link>
      <pubDate>Thu, 30 May 2024 20:50:00 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/swe_agent_llm_software_engineering/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2405.15793&#34;&gt;https://arxiv.org/abs/2405.15793&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 6. May 2024&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; LLM, Agents, Software-Engineering&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;This paper introduces SWE-agent, which is an autonomous system that uses LLM to fix real life GitHub issues. The Agent is tested on SWE-Benchmark and achieves a success rate of 12.5%.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;In recent times, LLMs have become very popular tools in software development, mostly to generate small snippets of code and auto completion. In these cases the human serves as mediator between the problem and the LLM. For instance providing error messages to refine the code.&#xA;In this paper, the authors introduce SWE-Agent which is an autonomous system, bases on a LLMN, that solves real world Software-Engineering problems.&#xA;For that it outputs thoughts and actions(ReAct) and receives feedback from the environment based on it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AutoCodeRover: Autonomous Program Improvement</title>
      <link>http://localhost:1313/paper-summary/autocoderover_llm_github/</link>
      <pubDate>Wed, 17 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/autocoderover_llm_github/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; AutoCodeRover: Autonomous Program Improvement&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.05427&#34;&gt;https://arxiv.org/abs/2404.05427&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 8. April 2024&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; LLM, Code-generation&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt; &lt;br&gt;&#xA;LLMs have already been extensively used in easing the workload of developers. In this paper the authors proposed AutoCodeRover a framework to automatically solve GitHub issues. In this framework LLMs are combined with code search algorithm&amp;rsquo;s. They use Abstract syntax trees instead of rare files.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Automating software developing has for a long time been a vision of engineers. Specifically, the authors of the paper, focused on bug fixing and feature addition.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RoCo: Dialectic Multi-Robot Collaboration with Large Language Models</title>
      <link>http://localhost:1313/paper-summary/roco_robots_with_llm/</link>
      <pubDate>Sun, 14 Apr 2024 09:30:00 +0200</pubDate>
      <guid>http://localhost:1313/paper-summary/roco_robots_with_llm/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; RoCo: Dialectic Multi-Robot Collaboration with Large Language Models &lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.04738&#34;&gt;https://arxiv.org/abs/2307.04738&lt;/a&gt; &lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 10. July 2023&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; NLP, LLM, Robots&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt; &lt;br&gt;&#xA;The goal of this paper is to improve multi-robot collaboration trough harnessing the power of LLM. For that they equip the robots with a LLM to discuss their task and form strategies. The LLMs form strategies through the generation of sub-tasks, which are then transformed to space waypoints. The space waypoints are used by motion planner to generate trajectories for the robot arms.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent</title>
      <link>http://localhost:1313/paper-summary/rest_meets_react/</link>
      <pubDate>Sat, 06 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/rest_meets_react/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; REFINER: Reasoning Feedback on Intermediate Representations&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.10003&#34;&gt;https://arxiv.org/abs/2312.10003&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 15. December 2023&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; LLM, LLM-Agent&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;This paper introduces an LLM-Agent using the ReAct framework combined with their new technique ReST which iteratively train using RL.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;For simple task such as question answering one can just ask the LLM and it works relative good, but for more complex tasks prompting directly isn&amp;rsquo;t good enough. Instead lately LLM-Agents have become very popular for this, which decompose the complex task into multiple smaller ones, often using external tools and APIs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>REACT : SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS</title>
      <link>http://localhost:1313/paper-summary/react_llm_agents/</link>
      <pubDate>Thu, 04 Apr 2024 14:29:16 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/react_llm_agents/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; ReAct: Synergizing Reasoning and Acting in Language Models&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.03629&#34;&gt;https://arxiv.org/abs/2210.03629&lt;/a&gt;  &lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 6. October 2022&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; LLM, LLM-Agent&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt; &lt;br&gt;&#xA;They introduce the technique ReAct which allows LLMs to generate both reasoning and task-specific actions, by combining actions with language.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;One thing that makes out human intelligence is the ability of combining task-related action with verbal language reasoning.&#xA;For example we might use language to keep track of our current progress.&#xA;This forms a tight connection between &amp;ldquo;reasoning&amp;rdquo; and &amp;ldquo;acting&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Improving Factuality and Reasoning in Language Models through Multiagent Debate</title>
      <link>http://localhost:1313/paper-summary/multi_agent_debate_llm/</link>
      <pubDate>Wed, 03 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/multi_agent_debate_llm/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Improving Factuality and Reasoning in Language Models through Multiagent Debate &lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.14325&#34;&gt;https://arxiv.org/abs/2305.14325&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 23. May 2023 &lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; NLP, LLM, RL, LLM-Agents&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;This paper investigates the usage of multiple LLMs as agent that debate between each other, to improve the quality of responses and reduce hallucinations.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;There have been done much research into how to increase the capabilities of LLMs and increase their responses in quality and accuracy. But much of the research has focused on how to improve a single LLM or &lt;em&gt;a single mind&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Siren’s Song in the AI Ocean:A Survey on Hallucination in Large Language Models</title>
      <link>http://localhost:1313/paper-summary/sirens_song_llm/</link>
      <pubDate>Tue, 02 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/sirens_song_llm/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Siren&amp;rsquo;s Song in the AI Ocean: A Survey on Hallucination in Large Language Models&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.01219&#34;&gt;https://arxiv.org/abs/2309.01219&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 3. September 2023&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; LLM, Hallucination&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt; &lt;br&gt;&#xA;LLM&amp;rsquo;s often exhibit so called &amp;ldquo;Hallucination&amp;rdquo;, this paper gives an overview of the different types of Hallucination and mitigation techniques.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;It has been observed that LLM&amp;rsquo;s often times produce outputs that while plausible, conflict with the user input, with previous context, or with factual knowledge.&#xA;This behavior is commonly called Hallucination, and hinder the reliability of the LLM.&#xA;Analyzing the problem id made difficult by:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Training language models to follow instructions with human feedback</title>
      <link>http://localhost:1313/paper-summary/training_language_models_to_follow_instructions/</link>
      <pubDate>Mon, 05 Feb 2024 06:45:55 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/training_language_models_to_follow_instructions/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Training language models to follow instructions with human feedback&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.02155&#34;&gt;https://arxiv.org/abs/2203.02155&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 4. March 2022&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; NLP, LLM, Instruction, Alignment&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;This paper introduces a novel technique, instruction fine-tuning, to enhance the alignment of Language Models (LLM) with user instructions. The approach utilizes reinforcement learning from human feedback (RLHF) and demonstrates its effectiveness by applying it to GPT-3.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Language Models (LLM) can be prompted for natural language tasks, but often, the results deviate from the given instructions. This misalignment occurs because LLMs are trained to predict the next token in their input rather than to follow specific tasks. To address this issue, the authors propose instruction fine-tuning, employing reinforcement learning from human feedback (RLHF) to better align the model&amp;rsquo;s objective with the intended task. This technique utilizes human preferences as a reward signal to fine-tune the models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Constitutional AI: Harmlessness from AI Feedback</title>
      <link>http://localhost:1313/paper-summary/constitutional_ai_harmlessness_from_ai_feedback/</link>
      <pubDate>Mon, 05 Feb 2024 06:15:42 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/constitutional_ai_harmlessness_from_ai_feedback/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Constitutional AI: Harmlessness from AI Feedback&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.08073&#34;&gt;https://arxiv.org/abs/2212.08073&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 15. December 2022&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt;  LLM, Alignment, fine-tuning&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;As AI models become more advanced, the need for alignment with human goals, specifically in terms of harmlessness and helpfulness, becomes crucial. Constitutional AI presents a model aligned through a constitution, employing both supervised and RL training methodologies.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;The authors aim to train an AI model that is both harmless and helpful, acknowledging the impracticality of manual supervision and advocating for an automated approach.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mathematical discoveries from program search with large language models</title>
      <link>http://localhost:1313/paper-summary/mathematical_discoveries_from_program_search_with_llm/</link>
      <pubDate>Sun, 04 Feb 2024 17:44:58 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/mathematical_discoveries_from_program_search_with_llm/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Mathematical discoveries from program search with large language models&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://www.nature.com/articles/s41586-023-06924-6&#34;&gt;https://www.nature.com/articles/s41586-023-06924-6&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 14. December 2023&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; LLM, RL, Coding&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;The authors of this paper introduce &lt;em&gt;FunSearch&lt;/em&gt;, an evolutionary algorithm using LLM with an evaluator to solve combinatoric problems and make new mathematical discoveries. This marks the first instance where novel mathematical knowledge was uncovered through the assistance of a language model.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Numerous NP-Problems are challenging to solve, yet they possess a polynomial evaluation algorithm. This paper focuses on such problems, aiming to generate &lt;em&gt;solve programs&lt;/em&gt; that receive high scores from the polynomial evaluator.&lt;/p&gt;</description>
    </item>
    <item>
      <title>REFINER: Reasoning Feedback on Intermediate Representations</title>
      <link>http://localhost:1313/paper-summary/refiner/</link>
      <pubDate>Sun, 04 Feb 2024 14:29:16 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/refiner/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; REFINER: Reasoning Feedback on Intermediate Representations&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.01904&#34;&gt;https://arxiv.org/abs/2304.01904&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 4. April 2023&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; LLM, prompting&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;LLMs have demonstrated remarkable performance on reasoning tasks. In this paper, the authors introduce REFINER, a framework for fine-tuning LLMs to generate explicit reasoning steps.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Numerous papers have highlighted the importance of generating explicit reasoning steps for improving a model&amp;rsquo;s performance and interpretability. However, these steps can be unreliable or incorrect. Typically, the approach to solve this, involves annotating new data with fixed errors and fine-tuning the model on this data, a process that demands significant computational resources and time.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reflexion: Language Agents with Verbal Reinforcement Learning</title>
      <link>http://localhost:1313/paper-summary/reflexion_language_agents_with_verbal_reinforcement/</link>
      <pubDate>Sun, 04 Feb 2024 13:54:38 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/reflexion_language_agents_with_verbal_reinforcement/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Reflexion: Language Agents with Verbal Reinforcement Learning&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.11366&#34;&gt;https://arxiv.org/abs/2303.11366&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 20. March 2023&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; LLM, RL, Agent, prompting&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;LLMs have increasingly interacted with different environments (e.g., APIs, games). However, learning through Reinforcement Learning (RL) is not efficient because it requires many trial-and-error iterations. The author proposes a new technique called Reflexion, which seeks to remedy this.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Recent papers have shown the effectiveness of using LLMs as decision-making agents in external environments. In these papers, LLMs generate text based on observations of the world. This generated text is then interpreted as an action of the agent and executed. The problem is that these LLM agents need to be trained, but using traditional RL would require too many trial-and-error iterations and a high amount of computational resources.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Self-Instruct: Aligning Language Models with Self-Generated Instructions</title>
      <link>http://localhost:1313/paper-summary/self_instruct__aligning_language_models/</link>
      <pubDate>Fri, 02 Feb 2024 11:02:28 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/self_instruct__aligning_language_models/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Self-Instruct: Aligning Language Models with Self-Generated Instructions&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;https://arxiv.org/abs/2212.10560&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 20. December 2022&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; LLM, Alignment, Instruction&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;Language Models (LLM) that are &amp;ldquo;instruction tuned&amp;rdquo; have demonstrated excellent raw performance and adherence to instructions. However, instruction tuning typically requires substantial amounts of human-labeled data. This paper introduces Self-Instruct, a technique for instruction tuning a LLM without relying extensively on human-labeled data.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Recent advancements in Language Models (LLM) have significantly enhanced their performance, attributed to the use of pre-trained LLM and subsequent fine-tuning (as seen in the InstructGPT paper) on human-written instructional data (e.g., SUPER-NATURALINSTRUCTIONS). The challenge lies in the impracticality of collecting large datasets, often lacking diversity in the provided instructions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Toolformer: Language Models Can Teach Themselves to Use Tools</title>
      <link>http://localhost:1313/paper-summary/toolformer/</link>
      <pubDate>Thu, 01 Feb 2024 20:56:12 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/toolformer/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Toolformer: Language Models Can Teach Themselves to Use Tools&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2302.04761&#34;&gt;https://arxiv.org/abs/2302.04761&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 9. Feburary 2023&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; NLP, LLM&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;In this paper, the author demonstrates that Language Models (LLMs) can autonomously learn to use external tools through simple APIs. They introduce the Toolformer model, which is trained to showcase this unique ability.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;While LLMs excel in various Natural Language Processing (NLP) tasks, they face limitations, including the inability to access real-time information, the tendency to hallucinate facts, a lack of precise mathematical skills, and a lack of awareness of temporal progression.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</title>
      <link>http://localhost:1313/paper-summary/chain_of_thought_prompting_elicits_reasoning_in_llms/</link>
      <pubDate>Thu, 01 Feb 2024 13:16:08 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/chain_of_thought_prompting_elicits_reasoning_in_llms/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Chain-of-Thought Prompting Elicits Reasoning in Large Language Models&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2201.11903&#34;&gt;https://arxiv.org/abs/2201.11903&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 28. January 2022&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; NLP, LLM, prompting&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;The paper explores the concept of Chain of Thought, a sequence of intermediate reasoning steps that significantly enhances the reasoning abilities of Large Language Models (LLMs).&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Chain of thought is an combination of two key ideas:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Arithmetic reasoning, can benefit from using some kind of rationale which lead to the final answer.&lt;/li&gt;&#xA;&lt;li&gt;LLM&amp;rsquo;s have the ability of in context learning, this means without changing their parameters, by using multiple examples in their prompts.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;In essence, Chain of Thought represents a series of reasoning steps in natural language leading to the solution of a given task.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Super-NaturalInstructions: Generalization via Declarative Instructions on 1600&#43; NLP Tasks</title>
      <link>http://localhost:1313/paper-summary/super_naturalinstructions/</link>
      <pubDate>Thu, 01 Feb 2024 08:40:19 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/super_naturalinstructions/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.07705&#34;&gt;https://arxiv.org/abs/2204.07705&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 16. April 2022&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; NLP, LLM, Instruction&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;In this paper, the authors investigate the generalization capabilities of NLP models to unseen tasks.&lt;br&gt;&#xA;It&amp;rsquo;s one of the first papers that uses instructions finetuning.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;This paper constructs a meta-dataset called &lt;strong&gt;SUPER-NATURALINSTRUCTIONS&lt;/strong&gt;, a large benchmark comprising 1616 NLP tasks along with their natural language instructions. Each task in the dataset is accompanied by an instruction that describes the task for the model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mistral_7B</title>
      <link>http://localhost:1313/paper-summary/mistral_7b/</link>
      <pubDate>Wed, 31 Jan 2024 20:36:46 +0000</pubDate>
      <guid>http://localhost:1313/paper-summary/mistral_7b/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper Title:&lt;/strong&gt; Mistral 7B&lt;br&gt;&#xA;&lt;strong&gt;Link to Paper:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.06825&#34;&gt;https://arxiv.org/abs/2310.06825&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Date:&lt;/strong&gt; 10. Oct 2023&lt;br&gt;&#xA;&lt;strong&gt;Paper Type:&lt;/strong&gt; NLP, finetuning, LLM-training&lt;br&gt;&#xA;&lt;strong&gt;Short Abstract:&lt;/strong&gt;&lt;br&gt;&#xA;Introduction of the &amp;ldquo;Mistral 7B&amp;rdquo; LLM model.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Mistral 7B deliver high performance while maintaining an efficient inference, it outperforms the previous best 13B Models on all tested benchmarks.&lt;/p&gt;&#xA;&lt;p&gt;It does this by using grouped-query attention (GQA) and sliding window attention (SWA), GQA accelerates inference speed and SWA allows the Model to work with longer sequences.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
