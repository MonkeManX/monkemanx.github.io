<!DOCTYPE html>
<html>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
      
      <a id="top"></a>
      <meta charset="utf-8">
      <meta name="viewport" 
          content="width=device-width, initial-scale=1.0">
      <title>My site</title>
      <link rel="stylesheet" href="/styles/style.css">  

      <link rel="stylesheet" href="/css/syntax.css">
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    }
  };
</script>

  </head>
  <body><div class="nav-div">
  <nav>
        <a href="http://localhost:1313/" class="">Home</a>
        <a href="http://localhost:1313/articles/" class="">Articles</a>
        <a href="http://localhost:1313/paper-summary/" class="active">Paper-Summaries</a>
        <a href="http://localhost:1313/tags/" class="">Tags</a>
        <a href="http://localhost:1313/poetry/" class="">Writings</a>
        <a href="http://localhost:1313/tangled_thoughts/" class="">Tangled-Thoughts</a>
        <a href="http://localhost:1313/media/" class="">Media</a>
        <a href="http://localhost:1313/links/" class="">Links</a>
        <a href="http://localhost:1313/about/" class="">About</a>
  </nav>
</div>
<div id="content">
<div class="article-content"> 
  <h1> RoCo: Dialectic Multi-Robot Collaboration with Large Language Models </h1>
  <p class="pub-date">Published: April 14, 2024</p>
  
  <p><strong>Paper Title:</strong> RoCo: Dialectic Multi-Robot Collaboration with Large Language Models <br>
<strong>Link to Paper:</strong> <a href="https://arxiv.org/abs/2307.04738">https://arxiv.org/abs/2307.04738</a> <br>
<strong>Date:</strong> 10. July 2023<br>
<strong>Paper Type:</strong> NLP, LLM, Robots<br>
<strong>Short Abstract:</strong> <br>
The goal of this paper is to improve multi-robot collaboration trough harnessing the power of LLM. For that they equip the robots with a LLM to discuss their task and form strategies. The LLMs form strategies through the generation of sub-tasks, which are then transformed to space waypoints. The space waypoints are used by motion planner to generate trajectories for the robot arms.</p>


<figure>
    <img style="display: block; margin-left: auto; margin-right: auto; width:90%" src="/attachments/20240414_09h39m34s_grim.png">
</figure>


<h1 id="1-introduction">1. Introduction</h1>
<p>Multi-robot system, such as multiple robots working at a assembly line, are interesting for their promise of enhancing productivity.
But they have multiple challenges to overcome:</p>
<ul>
<li>High understanding of the task, in order to be able to split the task up.</li>
<li>Understanding the capabilities of the robots, such as reach or payload.</li>
<li>Low-level motion planning, finding collision free paths.</li>
<li>Generality, most of the times multi-robot systems need task-specific engineering.</li>
</ul>
<p>The zero-shot method of the author called <strong>RoCo</strong>, consist of three components:</p>
<ul>
<li><strong>Dialogue-style task-coordination:</strong> They let the robots &rsquo;talk&rsquo; among themselves, by assigning to each robot a LLM. This should help for task reasoning.</li>
<li><strong>Feedback-improved Sub-task Plan Generated by LLMs:</strong> The dialogue of the LLMs ends with a sub-task plan for each agent, they use validation methods to check for the validity of the sub task and give the LLM feedback.</li>
<li><strong>LLM-informed Motion Planning in Joint Space:</strong> From the validated sub-task, they use RRT-sampler to plan motion trajectories.</li>
</ul>
<p>Furthermore they introduce <em>RoCoBench</em>, a benchmark which test the robots on 6 multi-robot manipulation tasks.</p>
<h2 id="2-preliminaries">2. Preliminaries</h2>
<p><strong>Task Assumptions:</strong></p>
<ul>
<li>Cooperative multi task agent enviroment.</li>
<li>\(N\) Robots.</li>
<li>\(T\) finite time horizon.</li>
<li>\(O\) Observation space.</li>
<li>An agent \(n\) has observation space \(\Omega^n \subset O\).</li>
<li>Description function \(f\) that translates task semantics and observations at time step \(t\) to natural language prompts: \(l_{t}^{n}=f^{n}(o_t), o_t \in \Omega^{n}\).</li>
</ul>
<p><strong>Multi-arm Path Planning:</strong></p>
<ul>
<li>\(X \in \mathbb{R}^d\) is the joint configuration space of all \(N\) Robot arms.</li>
<li>\(X_{ob}\) the obstacle region in the configuration space.</li>
<li>\(X_{free} = X \backslash X_{ob}\) collision free space.</li>
<li>The motion planner finds a optimal $\sigma^{*} : [0, 1] \to X$ with \(\sigma^{*}(0) = X_{init}\) and \(\sigma^{*}(1) = X_{goal}\).</li>
<li>\(\sigma^{*}\) is then used by the robot arms.</li>
</ul>
<h2 id="3-multi-robot-collaboration-with-llms">3. Multi-Robot Collaboration with LLMs</h2>
<h2 id="31-multi-agent-dialog-via-llms">3.1 Multi-Agent Dialog via LLMs</h2>
<p>Before, each environment interaction, the robot arms will do an round of dialog where each robot has a LLM assigned to it, which receives information and responds to it.</p>


<figure>
    <img style="display: block; margin-left: auto; margin-right: auto; width:90%" src="/attachments/20240414_10h07m08s_grim.png">
</figure>


<p>Each agent gets the same LLM prompt structure, but with different content:</p>
<ol>
<li><strong>Task Context:</strong> Describes the objectives of the task.</li>
<li><strong>Round History:</strong> Past Dialogue and executed actions.</li>
<li><strong>Agent capabilities:</strong> The Agent skills.</li>
<li><strong>Communication Instructions:</strong> How to responds to the other agents.</li>
<li><strong>Current Observation:</strong> What the agent is currently &lsquo;seeing&rsquo;.</li>
<li><strong>Plan Feedback:</strong>(optional) Reasons why a sub-task plan failed.</li>
</ol>
<p>Each agent is asked to end the response with either deciding to 1) continue the discussion or 2) summarize everyone actions and make a final proposal.
The second option is only allowed if every agent responded at least once.</p>
<h2 id="32-llm-generated-sub-task-plan">3.2 LLM-Generated Sub-task Plan</h2>
<p>After the discussion ends, the agent needs to summarize the results and make a &lsquo;sub-task plan&rsquo;, where each agent gets a sub-task(e.g. pick up a object) and a 3D waypoint.
Before execution the sub-task plans are validated, if a check fails the feedback is appended to the agent prompt an another round of discussion starts.
Following validation have to be passed:</p>
<ol>
<li><strong>Task parsing</strong> plan follows the desired format.</li>
<li><strong>Task Constrains</strong> check if the plan complies with the robots capabilities.</li>
<li><strong>IK</strong> checks whether a robot arm position is feasible via iinverse kinemtics.</li>
<li><strong>Collision Checking</strong> check if the robot arm position will cause a collision.</li>
<li><strong>Valid Waypoints</strong> if a task requires path planning, each intermediate waypoints must pass <em>IK</em> and <em>Collision Check</em>.</li>
</ol>
<p>A task is considered to be failed, if a maximum amount of discussion was reached.</p>
<h2 id="33-llm-informed-motion-planning">3.3 LLM-informed Motion Planning</h2>
<p>Once all validation have been passed , they are combined with <em>IK</em> to produce a final goal configuration for all robot arms.
The goal is than passed to an RRT-based multi arm motion planner that plans for all arms and outputs motion trajectories for each arm.</p>
<h1 id="4-benchmarks">4. Benchmarks</h1>
<p>The RoCoBench benchmark consist of 6 multi-robot collaboration in a tabletop setting. Each task has three key properties:</p>
<ol>
<li>Task decomposition  whether a task can be decomposed into sub-tasks.</li>
<li>Observation space: how much of the task is shared.</li>
<li>Workspace overlap: proximity between robots.</li>
</ol>
<h1 id="5-experiments">5. Experiments</h1>
<p>They evaluate following methods:</p>
<ul>
<li>RoCo: Previous described Method</li>
<li>In &lsquo;Central Plan&rsquo; the LLM-agent is given full knowledge of the enviroment.</li>
<li>In &lsquo;Dialog w/o History&rsquo; dialogue from past round is removed.</li>
<li>In &lsquo;Dialog w/o Feedback&rsquo; failed plans are discarded and agent try again without feedback.</li>
</ul>


<figure>
    <img style="display: block; margin-left: auto; margin-right: auto; width:90%" src="/attachments/20240414_10h27m11s_grim.png">
</figure>


<p>LLM-proposed 3D waypoints show no clear benefit for picking sub-taks, but accelerate planning.</p>
<p>RoCo is strongly adaptable in:</p>
<ul>
<li>Object Initialization: Randomizing objects.</li>
<li>Task Goal: Changing the goal.</li>
<li>Robot Capability: Changing of capabilities.</li>
</ul>


<figure>
    <img style="display: block; margin-left: auto; margin-right: auto; width:90%" src="/attachments/20240414_10h36m37s_grim.png">
</figure>


<p>They validate RoCo in a real World Setup, where the robot arm needs to collaborate with a human to complete the task.
For perception they use a pre-trained object detection model, OWL-ViT, to generate scene descriptions.</p>


<figure>
    <img style="display: block; margin-left: auto; margin-right: auto; width:40%" src="/attachments/20240414_10h36m44s_grim.png">
</figure>


<h1 id="6-multi-agent-reasoning-dataset">6. Multi-Agent Reasoning Dataset</h1>
<p>In addition to their man result, they introduce a text-based benchmark called, RoCoBench-Text, to evaluate an LLM&rsquo;s agent reasoning ability.</p>


<figure>
    <img style="display: block; margin-left: auto; margin-right: auto; width:90%" src="/attachments/20240414_10h42m51s_grim.png">
</figure>


<h1 id="7-conclusion">7. Conclusion</h1>
<p>Limitation:</p>
<ul>
<li>RoCo assumes object detection is accurate.</li>
<li>Motion trajectories can lead to errors.</li>
<li>Querying LLMs is expensive.</li>
</ul>
<p>RoCo is a new framework for collaboration of multiple robots with each other to solve tasks.</p>
 
</div>
<a href="#top" class="back-to-top-button">
  &#9650; 
</a>

    </div><div class="footer">
  Made with <a href="https://gohugo.io/">Hugo</a>, website licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>.
</div>
</body>
</html>
